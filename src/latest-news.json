[{"title":"White House Announces COVID-19 High Performance Computing Consortium","image":"/images/cdc-w9keokhajkw-unsplash.jpg","content":"On March 23, 2020, The White House launched the **COVID-19 High Performance Computing Consortium**, a new program that offers COVID-19 researchers access to high performance computing systems. The use of these systems aims to greatly expedite the timelines for calculations and modeling required for the research of the virus.\n\nCreated by the White House Office of Science and Technology Policy, the US Department of Energy, and IBM, this partnership brings together the public and private sectors to provide resources for research into COVID-19. Consortium members include leaders in industry and academia, as well as federal agencies and the US Department of Energy National Laboratories, all of which have donated free compute time and resources on their high performance computing systems.\n\nDetails about the consortium and statements from its members can be found here: <https://www.whitehouse.gov/briefings-statements/white-house-announces-new-partnership-unleash-u-s-supercomputing-resources-fight-covid-19/>\n\nThe consortium is currently accepting research proposals related to COVID-19 on its online portal. Information on the consortium’s computing resources and instructions for submitting proposals can be found here: <https://covid19-hpc-consortium.org/>\n\nUSC Research Computing is committed to supporting USC researchers in the effort of fighting COVID-19. We are offering our resources for research related to the virus, and we welcome any possible partnerships for proposal submission to the consortium. Researchers in bioinformatics, epidemiology, molecular modeling, and any other relevant areas are invited to contact us at carc-support@usc.edu to find out more about the program.\n"},{"title":"Updates to Project File System Storage Allocations","image":"/images/shahadat-rahman-bfrqnkbulyq-unsplash-1-.jpg","content":"With our new project file system comes a change to our storage allocation quota and policy.\n\nStorage allocations are measured using disk space in terabytes (equivalent to 1,000 gigabytes).\n\nAll users of active projects automatically receive a storage allocation on the CARC project file system. The default allocation size is 1 TB per PI, which can be increased up to 5 TB per PI, if needed, at no extra cost. **Unlike compute allocations, the 5 TB storage limit is per PI, rather than per project**. If a PI has multiple projects, the maximum allocation of 5 TB can be divided among the projects.\n\nIf more than 5 TB is needed, a PI can request additional storage space through the [Research Computing User Portal](https://hpcaccount.usc.edu/). The additional storage space in excess of the first 5 TB is a paid service. The current price is $40/TB/year.\n\nFor example, when a PI requires 10 TB of space, the annual charge for the data storage space will be:\n\n> 10 TB/year = 5 TB free + (5 x $40/TB/year = $200/year)\n\nFor more information on allocations, see our [Accounts and Allocations page](https://carc.usc.edu/user-information/accounts)."},{"title":"\"Upcoming Changes for USC Research Computing","image":"/images/9042079701_82c9f20225_c.jpg","content":"For the first half of 2020, USC Research Computing has been working on many different projects to improve its service quality and the experience for our users. While some of our legacy systems have been experiencing issues recently, the Research Computing team has been working relentlessly to deliver improved high-performance computing systems and services that will change the computing environment, user support and education, and overall business model of USC Research Computing. There are many changes scheduled to be announced during the summer and the fall 2020 semester. Below is a summary of the many exciting changes in the works.\n\n1. **A few improvements that have already been completed:**\n\n•\t /rcf-proj2 & /rcf-proj3: Our old project file system had a stability issue with NSF version 4 and its old Solaris platform, and this was causing frequent system downtime. In order to keep users’ data safe, we had to make the file system read-only in April.\n\n•\t/scratch (800TB): This is a new parallel file system running ZFS/BeeGFS. It was originally planned to be a temporary scratch file system with a data purging policy, but it is now serving as a semi-permanent data storage space until the new /project file system is available.\n\n•\t/staging (230TB): The old staging file system (230TB) was retired in early June, as the storage space wasn’t large enough to keep up with users’ needs. We have since upgraded /staging to another parallel file system called /scratch2.\n\n•\t/scratch2 (700TB): This is another ZFS/BeeGFS parallel file system available as a scratch space. At the moment, the system is available for long-term data storage without a data purging policy. The purging will resume once the new project file system is available and data migration is completed.\n\n•\thpc-transfer upgrade: The old hpc-transfer was decommissioned. A new system is now connected to a faster uplink switch with higher bandwidth (100GE) and low latency. We plan to add a few more data transfer tools, such as GlobusConnect and IBM Aspera.\n\n2. **New cluster: Discovery**\n\nOur new Research Computing cluster, Discovery, will debut in July to serve the USC research community. The new system comes with many changes that will provide better usability and a more sophisticated application layer. Noticeable improvements include:\n\n•\t/home directory: Every user on Discovery will get a 100GB allocation of home directory space. The home directory provides 2-week snapshots with daily backups, so if you accidentally delete some of your important data, you can recover the data if the deletion was within the last three weeks.\n\n•\tRebuilt software stack: We have rebuilt a couple hundred new applications on the cluster, and the new application stack was built with the Lmod/Spack module build system. Users can use available software via module loading, and necessary environment variables and other dependencies associated with libraries, compilers, and MPI stacks are automatically managed. More details and instructions will be available in our new user guides.\n\n•\tMulti-jobs on a single node: The new Slurm policy is now supporting multiple jobs running on a single node. So far, we have used a “single-job on a single-node” policy in order to avoid unexpected system crashes. With a new Slurm configuration, multiple jobs can share one compute node, increasing system efficiency and reducing waiting time in the queue significantly.\n\n3. **New Project file system**\n\nA new, high-performing parallel file system will be deployed this summer. This new storage system will provide 10PB of disk space and will replace the current /rcf-proj file systems. We expect this new file system will out-perform the current project file system with much more room for data. The pricing for this file system has not been finalized, but it will be within a very reasonable range and much more affordable than any of the public cloud storage services.\n\n4. **New Condo Cluster Program (CCP)**\n\nWe will continue to support the condo cluster system, but it will have some changes in its operation model. Rather than operating based on hardware ownership, we will utilize an annual subscription model. Researchers will be able to purchase computing resources (in the number of compute nodes or cores) for their annual needs, and they will be able to adjust the scale of their resources when renewing. The new model provides much more flexibility for users in terms of resource choices, and a much faster setup process without waiting through the entire system purchasing and installation process. The new business model and its new policies have been developed, and the new Condo Cluster Program will be launched in the fall semester of 2020.\n\n5. **New process for resource allocation management**\n\nWe are introducing a new allocation and project management process. It is similar to what we currently use, but with a few major differences:\n\n•\tUsers must be PhD-level researchers (faculty, postdoc, research scientists) to be eligible to be a PI.\n\n•\tEach PI will need to set up a project (or multiple projects) via an online user portal (more details in the next section), and PIs will be able to add/remove users for themselves.\n\n•\tOnce the project is set up, PIs can request resources (either compute nodes or storage space), and each resource should be associated with a PI’s project.\n\n•\tAllocations will have four tiers: small (up to 200K SUs**),* medium (up to 500K SUs), large (up to 1M SUs), and special (more than 1M SUs). Each tier will have a different review & approval process.\n\n\\*SU (System Unit): An hour of a CPU core time on a cluster system.\n\n6. **New website & user portal**\n\nA brand-new website for USC Research Computing will be launched in the summer of 2020. It comes with exciting services including all-new User Guides, an integrated Q&A platform for FAQs and discussions, and online user training and workshop materials. One of the most exciting features of the new website is the built-in user portal that will allows users to see their project information, allocation balance, and the system usage of their projects via a user dashboard. The project and allocation request and management process described in the previous section will be available as self-service features. Detailed information on system utilization and users’ job status will also be available based on each user’s project and allocation status on the system.\n\n7. **Data archiving service**\n\nFor those users who need an extra copy of their data, or even mirrored data archiving at another location (e.g., at Clemson University) for disaster recovery, we will offer a data archiving service for an extra fee. This data archiving service is jointly offered with the USC library and the Shoah Foundation, who is the world-leading expert in digital restoration and data archiving services. The data archiving service will be available from the fall semester of 2020.\n"},{"title":"USC’s Center for High-Performance Computing (HPC) is now the Center for","image":"/images/7305006266_e7851d0b45_c.jpg","content":"With the launch of its new high-performance computing cluster, USC’s Center for High-Performance Computing (HPC) has also officially changed its name to the Center for Advanced Research Computing (CARC).\n\nThis new name signifies the research support-focused direction the CARC is heading in. In addition to the new cluster and other system upgrades, the CARC will be expanding their services to include an updated, subscription-based condo node program, as well as a data archiving service. A new website, project resource user portal, and user education program will also be developed to support the research of USC’s faculty and students.\n\nA comprehensive list of the CARC’s system upgrades and upcoming changes can be found here: <https://carc.usc.edu/news-and-events/news-and-announcements/upcoming-changes-summer-fall-2020>\n"},{"title":"News","image":"/images/class-2020.png","content":"No content given"}]