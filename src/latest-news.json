[{"title":"White House Announces COVID-19 High Performance Computing Consortium","image":"/images/cdc-w9keokhajkw-unsplash.jpg","content":"On March 23, 2020, The White House launched the **COVID-19 High Performance Computing Consortium**, a new program that offers COVID-19 researchers access to high performance computing systems. The use of these systems aims to greatly expedite the timelines for calculations and modeling required for the research of the virus.\n\nCreated by the White House Office of Science and Technology Policy, the US Department of Energy, and IBM, this partnership brings together the public and private sectors to provide resources for research into COVID-19. Consortium members include leaders in industry and academia, as well as federal agencies and the US Department of Energy National Laboratories, all of which have donated free compute time and resources on their high performance computing systems.\n\nDetails about the consortium and statements from its members can be found here: <https://www.whitehouse.gov/briefings-statements/white-house-announces-new-partnership-unleash-u-s-supercomputing-resources-fight-covid-19/>\n\nThe consortium is currently accepting research proposals related to COVID-19 on its online portal. Information on the consortium’s computing resources and instructions for submitting proposals can be found here: <https://covid19-hpc-consortium.org/>\n\nUSC Research Computing is committed to supporting USC researchers in the effort of fighting COVID-19. We are offering our resources for research related to the virus, and we welcome any possible partnerships for proposal submission to the consortium. Researchers in bioinformatics, epidemiology, molecular modeling, and any other relevant areas are invited to contact us at carc-support@usc.edu to find out more about the program.\n"},{"title":"Exploring New Features on the CARC Website","image":"/images/home-page-small.png","content":"Announced in August 2020, the Center for Advanced Research Computing's new website is a major upgrade to USC's Research Computing program. In addition to its updated design, the website also includes numerous new features not seen on our previous website.\n\n[Home Page Resources](https://carc.usc.edu/)\n\n[](https://carc.usc.edu/)On our home page, you'll find a dashboard-style view of announcements, news stories, Tweets, helpful links, the current CARC system status, and a preview of the latest posts in our Discourse community. Check back frequently to see important announcements, exciting news stories and interviews, and to stay up-to-date on our Twitter and Discourse feeds!\n\n![](/images/columns.png)\n\n[News and Announcements](https://carc.usc.edu/news-and-events/news-and-announcements)\n\nOur news stories include CARC-specific content, including alerts about system maintenance and announcements of new or upgraded services, as well as community interest stories about research being done with CARC resources. In the coming months, we plan to feature interviews with some of our current researchers. Stay tuned!\n\n![](/images/news.png)\n\n[User Support](https://carc.usc.edu/user-support)\n\nOur User Support page is a first-point-of-contact for any and all problems or questions you might have.\n\nHere, you'll find links to our User Guides and Frequently Asked Questions pages to provide solutions for common questions or issues. This is also where you'll access our Ticket Submission page if you require help from a CARC staff member.\n\nWe've also provided links to our User Portal, where Principal Investigators (PIs) can manage their projects and resource allocations, and to our Discourse User Forum, which is a discussion community for CARC users (described below).\n\nIf you're experiencing issues with your USC NetID (which you use to log in to CARC systems) or Duo two-factor authentication, we've also provided a link to USC's IT Services help pages.\n\n![](/images/user-support.png)\n\n[Discourse User Forum](https://hpc-discourse.usc.edu/categories)\n\nThe Discourse discussion platform is a question-and-answer community for CARC users to ask questions, discuss research computing, and share their experiences with CARC systems and resources. It's a great place to seek out collaborative help from other users. CARC staff also post about troubleshooting and solutions to any common or unique issues our users have experienced.\n\n![](/images/discourse.png)"},{"title":"\"Upcoming Changes for USC Research Computing","image":"/images/slider-image-1-small.jpg","content":"For the first half of 2020, USC Research Computing has been working on many different projects to improve its service quality and the experience for our users. While some of our legacy systems have been experiencing issues recently, the Research Computing team has been working relentlessly to deliver improved high-performance computing systems and services that will change the computing environment, user support and education, and overall business model of USC Research Computing. There are many changes scheduled to be announced during the summer and the fall 2020 semester. Below is a summary of the many exciting changes in the works.\n\n1. **A few improvements that have already been completed:**\n\n•\t /rcf-proj2 & /rcf-proj3: Our old project file system had a stability issue with NSF version 4 and its old Solaris platform, and this was causing frequent system downtime. In order to keep users’ data safe, we had to make the file system read-only in April.\n\n•\t/scratch (800TB): This is a new parallel file system running ZFS/BeeGFS. It was originally planned to be a temporary scratch file system with a data purging policy, but it is now serving as a semi-permanent data storage space until the new /project file system is available.\n\n•\t/staging (230TB): The old staging file system (230TB) was retired in early June, as the storage space wasn’t large enough to keep up with users’ needs. We have since upgraded /staging to another parallel file system called /scratch2.\n\n•\t/scratch2 (700TB): This is another ZFS/BeeGFS parallel file system available as a scratch space. At the moment, the system is available for long-term data storage without a data purging policy. The purging will resume once the new project file system is available and data migration is completed.\n\n•\thpc-transfer upgrade: The old hpc-transfer was decommissioned. A new system is now connected to a faster uplink switch with higher bandwidth (100GE) and low latency. We plan to add a few more data transfer tools, such as GlobusConnect and IBM Aspera.\n\n2. **New cluster: Discovery**\n\nOur new Research Computing cluster, Discovery, will debut in July to serve the USC research community. The new system comes with many changes that will provide better usability and a more sophisticated application layer. Noticeable improvements include:\n\n•\t/home directory: Every user on Discovery will get a 100GB allocation of home directory space. The home directory provides 2-week snapshots with daily backups, so if you accidentally delete some of your important data, you can recover the data if the deletion was within the last three weeks.\n\n•\tRebuilt software stack: We have rebuilt a couple hundred new applications on the cluster, and the new application stack was built with the Lmod/Spack module build system. Users can use available software via module loading, and necessary environment variables and other dependencies associated with libraries, compilers, and MPI stacks are automatically managed. More details and instructions will be available in our new user guides.\n\n•\tMulti-jobs on a single node: The new Slurm policy is now supporting multiple jobs running on a single node. So far, we have used a “single-job on a single-node” policy in order to avoid unexpected system crashes. With a new Slurm configuration, multiple jobs can share one compute node, increasing system efficiency and reducing waiting time in the queue significantly.\n\n3. **New Project file system**\n\nA new, high-performing parallel file system will be deployed this summer. This new storage system will provide 10PB of disk space and will replace the current /rcf-proj file systems. We expect this new file system will out-perform the current project file system with much more room for data. The pricing for this file system has not been finalized, but it will be within a very reasonable range and much more affordable than any of the public cloud storage services.\n\n4. **New Condo Cluster Program (CCP)**\n\nWe will continue to support the condo cluster system, but it will have some changes in its operation model. Rather than operating based on hardware ownership, we will utilize an annual subscription model. Researchers will be able to purchase computing resources (in the number of compute nodes or cores) for their annual needs, and they will be able to adjust the scale of their resources when renewing. The new model provides much more flexibility for users in terms of resource choices, and a much faster setup process without waiting through the entire system purchasing and installation process. The new business model and its new policies have been developed, and the new Condo Cluster Program will be launched in the fall semester of 2020.\n\n5. **New process for resource allocation management**\n\nWe are introducing a new allocation and project management process. It is similar to what we currently use, but with a few major differences:\n\n•\tUsers must be PhD-level researchers (faculty, postdoc, research scientists) to be eligible to be a PI.\n\n•\tEach PI will need to set up a project (or multiple projects) via an online user portal (more details in the next section), and PIs will be able to add/remove users for themselves.\n\n•\tOnce the project is set up, PIs can request resources (either compute nodes or storage space), and each resource should be associated with a PI’s project.\n\n•\tAllocations will have four tiers: small (up to 200K SUs**),* medium (up to 500K SUs), large (up to 1M SUs), and special (more than 1M SUs). Each tier will have a different review & approval process.\n\n\\*SU (System Unit): An hour of a CPU core time on a cluster system.\n\n6. **New website & user portal**\n\nA brand-new website for USC Research Computing will be launched in the summer of 2020. It comes with exciting services including all-new User Guides, an integrated Q&A platform for FAQs and discussions, and online user training and workshop materials. One of the most exciting features of the new website is the built-in user portal that will allows users to see their project information, allocation balance, and the system usage of their projects via a user dashboard. The project and allocation request and management process described in the previous section will be available as self-service features. Detailed information on system utilization and users’ job status will also be available based on each user’s project and allocation status on the system.\n\n7. **Data archiving service**\n\nFor those users who need an extra copy of their data, or even mirrored data archiving at another location (e.g., at Clemson University) for disaster recovery, we will offer a data archiving service for an extra fee. This data archiving service is jointly offered with the USC library and the Shoah Foundation, who is the world-leading expert in digital restoration and data archiving services. The data archiving service will be available from the fall semester of 2020.\n"},{"title":"December Maintenance and 2020 Winter Recess Schedule","image":"/images/holiday_graphic.jpg","content":"\nAt the end of each year, USC departments cease operations for a winter recess period. This year, the CARC will be closed for the holidays beginning **Friday, December 25, 2020, reopening on Monday, January 4, 2021**. Non-emergency work will be paused until the new year. In the case of an emergency, tickets can still be submitted via our [ticket submission form](/user-information/ticket-submission).\n\nDuring this recess, USC Facilities Management Services will be performing a UPS (Uninterruptable Power Supply) system upgrade in the ITS data center. This upgrade affects all systems that utilize the data center, including CARC systems. The power system upgrade will commence on Sunday, December 27 at 8:00 pm. **All CARC systems, including Discovery, Endeavour, and the file systems, will be down during this maintenance period**. Please plan accordingly so your research work is not impacted. We will keep you updated on this event as we get more details before the winter recess.\n \nA special reservation in Slurm is configured not to take any jobs running past 8:00 pm on December 27. Users on Discovery do not need to worry about the runtime limit for new jobs (48 hours max), but Endeavour condo cluster users should note that new jobs that run longer than 3 weeks will not start until the completion of the upgrade. If you have any questions or concerns about this, please contact us by [submitting a help ticket](/user-information/ticket-submission).\n\nBecause this downtime will depend on the timeline by Facilities Management Services, we cannot guarantee on which day we will bring CARC systems back up. As soon as the upgrade is completed, we will then bring our systems back up, which we anticipate could take ~2 days. **At the latest, we expect to have our systems back up by January 6**. We will send out an announcement once the systems are accessible again. We apologize for such a long period of downtime, but this critical upgrade affects all systems that use the data center, including ours.\n\nFrom everyone at the Center for Advanced Research Computing, we hope you have a safe and warm holiday season. We look forward to continuing to support your research in 2021!\n"},{"title":"USC's Center for Advanced Research Computing (CARC) Officially Launches","image":"/images/slider-image-2-small.jpg","content":"With the launch of its new high-performance computing cluster, USC’s Center for High-Performance Computing (HPC) has also officially changed its name to the Center for Advanced Research Computing (CARC).\n\nThis new name signifies the research support-focused direction the CARC is heading in. In addition to the new cluster and other system upgrades, the CARC will be expanding their services to include an updated, subscription-based condo node program, as well as a data archiving service. A new website, project resource user portal, and user education program will also be developed to support the research of USC’s faculty and students.\n\nA comprehensive list of the CARC’s system upgrades and upcoming changes can be found here: <https://carc.usc.edu/news-and-events/news-and-announcements/upcoming-changes-summer-fall-2020>\n"},{"title":"CARC and Researchers Benefit from Reciprocal Partnerships","image":"/images/slider-image-1-small.jpg","content":"USC’s Center for Advanced Research Computing provides advanced computational research systems to the USC community. The CARC goes further than just providing resources, though: it also collaborates closely with the university’s wide variety of research groups, supporting their specialized needs and furthering their research.\n\nThroughout 2020 — and amid a worldwide pandemic — the CARC has continued to develop and strengthen its mutually-beneficial partnerships with USC’s researchers.\n\nThe CARC has a strong partnership with USC’s Information Sciences Institute (ISI), and with Carl Kesselman, Director, Informatics Division, in particular. Earlier this year, the CARC collaborated with Kesselman on a project proposal for a secure hybrid cloud system to support scientific research in Southern California. In August, the proposal was awarded a [$400,000 grant](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2019220&HistoricalAwards=false) from the National Science Foundation’s Campus Cyberinfrastructure program. In addition to this project, the CARC and Kesselman have plans to collaborate on future projects and research grant proposals.\n\nAnother researcher that the CARC has a close working relationship with is Andrew McMahon, founding Chair of the Department of Stem Cell Biology and Regenerative Medicine and Director of the Eli and Edythe Broad Center for Regenerative Medicine and Stem Cell Research at USC’s Keck School of Medicine. Working with large data sets in the range of 50 to 100 terabytes, McMahon’s lab utilizes CARC resources for bioinformatics analysis of next-generation sequencing data, including experiments to monitor protein-DNA interactions using ChIP-seq, a sequencing method combining chromatin immunoprecipitation (ChIP) with massively parallel sequencing.\n\nIvan Bermejo-Moreno, Assistant Professor of Aerospace and Mechanical Engineering at the Viterbi School of Engineering, uses CARC resources in his research of computational fluid mechanics and turbulent fluid flows. Of the CARC, Bermejo-Moreno says, “As a junior faculty working in the area of predictive science and engineering through simulation and numerical diagnostics, my group primarily relies on HPC \\[high-performance computing] resources, both at USC and at the National Laboratories, to advance our research.” In the spring, Bermejo-Moreno and his team had the opportunity to test the CARC’s Discovery cluster before it was released to the general user community, which provided invaluable feedback and quality assurance for the CARC.\n\nThese are only a few examples of the CARC’s commitment to its users and to the larger research community at USC. As its services are expanded and its systems continue to be upgraded, the CARC hopes to also increase its number of partnerships, particularly with academic fields that may not have benefited from research computing previously.\n\nFor more information on partnerships, see our [Research Partnerships page](https://carc.usc.edu/services/research-partnerships) and our [Research Profiles](https://carc.usc.edu/news-and-events/researcher-profiles)."},{"title":"Updates to Project File System Storage Allocations","image":"/images/slider-image-4-small.jpg","content":"With our new project file system comes a change to our storage allocation quota and policy.\n\nStorage allocations are measured using disk space in terabytes (equivalent to 1,000 gigabytes).  \n\nAll active projects automatically receive a storage allocation on the CARC project file system. The default minimum allocation size is 5 TB per project, and each PI receives a maximum allocation of 10 TB for their project(s) at no cost. All users for a project will have access to that project's allocation.\n\nIf more than 10 TB is needed, a PI can request additional storage space through the [Research Computing User Portal](https://hpcaccount.usc.edu/). The additional storage space in excess of the first 10 TB is a paid service. Additional storage space can be added in increments of 5 TB, and the current price is $200/5 TB/year. For example, when a PI requires 20 TB total of space, the annual cost for the data storage space will be:\n\n>20 TB per year = 10 TB free + (2 x $200/5 TB/year) = $400 annual cost\n\nFor more information on allocations, see our [Accounts and Allocations page](https://carc.usc.edu/user-information/accounts)."},{"title":"\"Scheduled CARC System Maintenance - September 18, 2020 \"","image":"/images/slider-image-5-small.jpg","content":"On September 18, our systems will undergo full-day maintenance, which will include improving Discovery’s interconnection network performance and installing our new project file system. System downtime will begin at 8:00 am and is projected to finish by the evening.\n\nDuring this time, all CARC systems, including the Discovery cluster, condo nodes, and our file systems, will be unavailable. Your data in our file systems will not be accessible during this time, but it will **not** be erased. The Slurm job scheduler will not be accepting jobs to run during the maintenance period. Once the maintenance is completed, we will send out an announcement.\n\nAfter the maintenance, we will be working with you to complete the data migration process from our current project file system to the new one. You will receive an email from us with instructions on how to migrate your data. Please look out for this email so you can begin your data migration process as soon as possible.\n\nAs always, please make sure that you have backed up any important data.\n\nMoving forward, we plan to have regular system maintenance scheduled for the third Friday of each month; September 18 will be the first occurrence of this regular maintenance."},{"title":"New Condo Cluster Program Officially Launches","image":"/images/slider-image-2-small.jpg","content":"\nThe CARC is pleased to announce its new [Condo Cluster Program (CCP)](/user-information/ccp/program-information), offering greater flexibility for researchers who prefer to have access to their own dedicated compute nodes.\n\nPrior to the development of the new CCP, PIs could purchase dedicated compute nodes using a traditional purchase model, which specified a five-year service term for the chosen nodes, after which the nodes would be retired. This pricing model remains, but the CCP also now includes an annual subscription pricing model. PIs can subscribe to their desired nodes on an annual basis, choosing either to renew, modify, or terminate their subscription each year. In both models, PIs have access to nodes that are completely dedicated for their own jobs, without the job limits and resource restrictions that the nodes in the general-use Discovery cluster have.\n\nPurchases and subscriptions can now be requested and managed in the [CARC user portal](/user-information/user-guides/research-computing-user-portal), much like Discovery and storage allocations. Subscriptions are requested in the same way as Discovery and storage allocations (see the [Request New Allocation user guide](/user-information/user-guides/research-computing-user-portal/request-new-allocation) for instructions). Purchases are requested via a new purchase form on the [Condo Purchase Requests page](https://hpcaccount.usc.edu/purchase/), under the “Project” tab (see the [Request New Condo Purchase user guide](/user-information/user-guides/research-computing-user-portal/request-new-purchase) for instructions).\n\nNodes subscribed to or purchased through the CCP form the [Endeavour condo cluster](/user-information/user-guides/high-performance-computing/getting-started-endeavour), which is a new high-performance computing cluster designated especially for the CCP. CCP users will enjoy the same advanced computing capabilities, fast speeds, and software stack that the Discovery cluster offers.\n\n**More information**\n\n* [Condo Cluster Program pages](/user-information/ccp)\n* [Subscription Pricing Model](/user-information/ccp/program-information/ccp-subscription)\n* [Purchase Pricing Model](/user-information/ccp/program-information/ccp-purchase)\n* [Getting Started with the Endeavour Condo Cluster user guide](/user-information/user-guides/high-performance-computing/getting-started-endeavour)\n* [Request New Condo Purchase user guide](/user-information/user-guides/research-computing-user-portal/request-new-purchase)\n\n"},{"title":"News","image":"/images/class-2020.png","content":"No content given"}]